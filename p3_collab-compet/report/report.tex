% Created 2020-04-10 Fri 06:20
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{unicode-math}
\author{Gianluca Scarpellini (gianluca@scarpellini.dev)}
\date{\today}
\title{Project 3 - Deep reinforcement learning - Collaboration and competition}
\hypersetup{
 pdfauthor={Gianluca Scarpellini (gianluca@scarpellini.dev)},
 pdftitle={Project 3 - Deep reinforcement learning - Collaboration and competition},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.3.6)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{Introduction}
\label{sec:orge7a8451}
Deep Reinforcement Learning is today becoming a relevant field of research. The
current work aims to evaluate the ability of value-based methods of solving a
simple task in a constrained environment. In particular, we implemented Deep DPG
algorithm as presented during Udacity "Deep Reinforcement Learning" as well as
in the original paper for 2 competitive agents. We describe the environment in
section \ref{sec:orgf68bd06}. A more in depth explanation of the approach is
presented in section \ref{sec:orgc2b7730}. In section \ref{sec:org09be8dc} we comment the
results obtained with the algorithm. Finally, we take our conclusions in section
\ref{sec:orgbd1153e}.


\section{Environment and task}
\label{sec:orgf68bd06}
The presented project is developed in order to solve the "Tennis" environment
with 2 agents in competition. The tasks consists in throwing the ball to
opposing competitor field. The reward is +0.1 for each right hit and a reward
of -0.01 for the wrong ones. The agents goal is to keep the ball in play as long
as possible. The observation space consists of 24 variables (position and
velocity of ball and racket). The action space is continuous and consists of a
vector of 2 continuous value corresponding to left-right movement and jump. The
task is episodic. The environment is considered solved if an average score of at
least \textbf{0.5} is maintained for 100 epochs.

\section{Algorithm and Implementation}
\label{sec:orgc2b7730}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{../contents/agentcritic.png}
\caption{\label{fig:org3b240ea}Actor-critic approach}
\end{figure}

We used an actor-critic approach to solve the environment. Actor-critic methods
merges the idea of \textbf{policy-based} and \textbf{temporal-differences} in order to
mitigate their issues both in variance and bias. DDPG exploits 2 different
function approximators (such as neural networks, CNN, \ldots{}). The policy produces
by the actor is \textbf{deterministic}: the action per state is the optimal one. The
critic evaluates the deterministic policy and updates its parameters using
TD-error. The \textbf{deterministic policy gradient algorithm} is employed to update
the actor weights following equation in figure \ref{fig:orgcd6f1f9}.


\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{../contents/dpg.png}
\caption{\label{fig:orgcd6f1f9}DPG equation}
\end{figure}

\subsection{Improvements}
\label{sec:orge234b3b}
We employed a few improvements proposed in the original DDPG article in order to
stabilize training.

\begin{itemize}
\item \textbf{Experience Replay}: The experience replay was originally proposed for Deep
Q-learning approach. It employs a buffer of finite size in order to sample
randomly from the state-action-rewards tuple space. Independent mini-batches
favor a more stable training. The states of both agents are pushed in the same
buffer for more stability.
\item \textbf{Soft updates}: In DDPG the target networks are updated using a \textbf{soft copy} of
the old weights. The result is a more smooth updating of neural networks
parameters.
\item \textbf{Parallelism}: We adapt DDPG in order to exploit a multi-agent
environment. The behavior of the algorithm is the same; the experience buffer
and the updates benefit from the high independence of the experiences coming
from different parallel agent.
\end{itemize}

\subsection{Hyperparameters}
\label{sec:org38f7ec7}
For the experiment we used the following hyperparameters:

\begin{center}
\begin{tabular}{lr}
Hyperparameter & Value\\
\hline
Replay buffer size & 1e6\\
Batch size & 1024\\
\(\gamma\) & 0.99\\
\(\tau\) & 1e-3\\
Actor lr & 1e-4\\
Critic lr & 1e-3\\
Update every & 20\\
Update times & 10\\
Episodes & 5000\\
\hline
\end{tabular}
\end{center}

\subsection{Neural Network}
\label{sec:org9460d00}
I implemented 2 neural networks as function approximators for both Actor and
Critic as summarized in tables \ref{tab:org266b26d} and \ref{tab:orgeb9921d}. The actor neural network is a Multi-layer perceptron with 1 hidden
layer. I used \textbf{leaky relu} as non-linearity for the input layer and the hidden
layer, while I used \textbf{tanh} for the ouputs of the last layer.  The critic network
is a Multi-layer perceptron too with multiple-input. I decided to insert a Batch
Normalization layer as initially proposed by the Deep Q-learning algorithm in
order to normalize the observations input. The first layer output is
concatenated with the actions input in order to merge the mutual information. 2
more hidden layers followed by leaky relu further modified the input space. The
linear layer as output is returned without non-linearity.




\begin{table}[htbp]
\caption{\label{tab:org266b26d}Actor network}
\centering
\begin{tabular}{lr}
\hline
Actort Network & Value\\
\hline
Fc1 & 256\\
fc2 & 128\\
\hline
\end{tabular}
\end{table}


\begin{table}[htbp]
\caption{\label{tab:orgeb9921d}Critic network}
\centering
\begin{tabular}{lr}
\hline
Critic Network & Value\\
\hline
fc1 & 256\\
fc2 & 128\\
fc3 & 128\\
\hline
\end{tabular}
\end{table}





\section{Results}
\label{sec:org09be8dc}
In figure \ref{fig:org15b44d9} it's presented the algorithm result per episode. In
particular, we were able to solve the environment in less then 50 epochs. The
scores kept growing until an optimal maximum of 2.0, probably due to limited
time play per episode. The learning was flat until when it started reaching a
local maximum. There was some noise in the precess, probably due the multi-agent
environment. Different approaches and improvements are discussed in section
\ref{sec:orgbd1153e}.

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{../contents/solved.png}
\caption{\label{fig:org15b44d9}Learning scores per epoch}
\end{figure}

\section{Conclusion and further}
\label{sec:orgbd1153e}
We developed a pipeline in order to solve `tennis` environment with Deep
Reinforcement Learning. In particular, we implemented \textbf{Deep Deterministic Policy
Gradient} following the paper specification. DDPG is an off-policy actor-critic
algorithm which has proven stability and optimal results in multiple tasks. We
believe better results in terms of training speed could be achievable using more
advances algorithms like PPO for continuous action. As a matter of fact, PPO
could better benefit from the parallelism offered by the environment. We can
improve the experience sampling by using a priority based sampling technique to
help the agent with the exploration/exploitation problem. More advance solutions
could involve D4PG algorithm or Hindsight Experience replay. 
\end{document}